---
title: "Bayesian Linear Regression"
author: "Luis M. Lomeli"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---


```{r, load-libraries, warning=FALSE}
library(ggplot2)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
require(plyr)
library(reshape2)
```


```{r, cache=TRUE}
plot(cars)
```

#Linear regression

## Least squares fit


```{r, least_squares, cache=TRUE}

least_squares = function(param, data){
  
  y_hat = param[1] + param[2]*data$x
  
  resid_squared = (y_hat - data$y)^2
  
  return(sum(resid_squared))
}

data = list(x=cars$speed, y=cars$dist)


least_squares(param=c(1,2), data=data)

fitted = optim(par=c(1,2), fn=least_squares, data=data, 
               hessian=TRUE,method="BFGS")

#Estimates
fitted$par


#standard errors
sqrt(diag(solve(fitted$hessian)))

```


## Linear regression with likelihood function

```{r, lm_loglik, cache=TRUE}
lm_loglik = function(param, data){
  
  mean = param[1] + param[2]*data$x
  
  loglik = sum(x=dnorm(data$y, mean=mean, sd=param[3], log=TRUE))
  
  return(-loglik)
  
}

lm_loglik(param=c(-10,2,1), data=data)

fitted_lm_loglik = optim(par=c(1,1,1), fn=lm_loglik, data=data)
fitted_lm_loglik$par

fitted$par

```




```{r, lm-plot, cache=TRUE}
x_pred = seq(min(data$x), max(data$x), length.out = 100)
y_pred = fitted$par[1] + fitted$par[2]*x_pred
df_pred = data.frame(x=x_pred, y=y_pred)

ggplot()+
  geom_point(data=as.data.frame(data), aes(x=x,y=y), colour="blue", size=2)  +
  geom_line(data = df_pred, aes(x=x_pred, y=y_pred), colour="blue", size=2) +
  theme_bw() 



```



```{r, lm-summary, cache=TRUE}

fit_lm = lm(dist ~ speed, data=cars)
summary(fit_lm)

```




## Stan Linear regression 


```{r, lm-STAN-basic, cache=TRUE}
library(rstan)


stan_lm = "
data {
  int<lower=0> N; // number of records
  vector[N] x; // predictor vector
  vector[N] y; // response vector
}

parameters {
  real beta0; // intercept
  real beta1; // slope
  real<lower=0> sigma; // noise error
}

model {
  y ~ normal(beta0 + beta1  * x, sigma); // likelihood
}

" 

stan_data = list(x=data$x, y=data$y, N=length(data$x))

stan_lm = stan(model_code=stan_lm, data=stan_data, iter=5000)

print(stan_lm)

pairs(stan_lm, pars=c('beta0', 'beta1', 'sigma'))



```

## Stan Linear regression predict

```{r, lm-STAN-pred, cache=TRUE}


stan_lm_pred = "
data {
  int<lower=0> N1; // number of records
  vector[N1] x; // predictor vector
  vector[N1] y; // response vector
  
  int<lower=0> N2; // number of records for prediction
  vector[N2] x_pred; // For prediction
}

parameters {
  real beta0; // intercept
  real beta1; // slope
  real<lower=0> sigma; // noise error
}

model {
  y ~ normal(beta0 + beta1  * x, sigma); // likelihood
}


generated quantities{
  vector[N2] y_pred_mean;
  real y_pred[N2];

  y_pred_mean = beta0 + beta1 * x_pred;
  
  y_pred = normal_rng(y_pred_mean, sigma);

}


" 

stan_data = list(x=data$x, y=data$y, N1=length(data$x), 
                 x_pred=x_pred, N2=length(x_pred))

stan_lm_pred = stan(model_code=stan_lm_pred, data=stan_data, iter=10000)

print(stan_lm_pred, pars=c('beta0', 'beta1', 'sigma'))

pairs(stan_lm_pred, pars=c('beta0', 'beta1', 'sigma'))

```


## Exercise: Write this code in vectorized way:

```{r, lm-STAN-EX-vect, cache=TRUE}

stan_lm_pred_vect = "
data {
  int<lower=0> N1; // number of records
  int<lower=0> K; // number of betas
  vector[N1, K] x; // predictors matrix
  vector[N1] y; // outcome vector
  
  int<lower=0> N2; // number of records for prediction
  vector[N2, K] x_pred // matrix of predictions
  
}

parameters {
  real alpha; // intercept
  vector[K] betas; // coefficients for predictors
  real<lower=0> sigma; // error scale
}

model {
  \\ No priors
  
  \\ Likelihood
  y ~ normal(x * betas , sigma); // likelihood
}


generated quantities{
  vector[N2] y_pred_mean;
  real y_pred[N2];

  y_pred_mean = betas * x_pred;
  
  y_pred = normal_rng(y_pred_mean, sigma);

}




" 





```


```{r, lm-STAN-plot, cache=TRUE}

samples = extract(stan_lm_pred)

y_pred_summ = summary(stan_lm_pred, pars=c('y_pred'))$summary
# y_hat = as.numeric(y_hat_summ$summary[,"mean"])


df_pred = data.frame(x_pred=x_pred,
                     ymin = as.numeric(y_pred_summ[,'2.5%']),
                     ymax = as.numeric(y_pred_summ[,'97.5%']),
                     ymedian = as.numeric(y_pred_summ[,'50%']),
                     ymean = as.numeric(y_pred_summ[,'mean'])
                    )
#Add predictions from linear regression model
df_pred$y_pred_lm = predict(fit_lm, list(speed=df_pred$x_pred))

head(df_pred)

TOT_SAMPLES = dim(samples$y_pred_mean)[1]

#Generate data frame with individual fitted lines aka spaguetti plots
TOT_SPAGUETTIS = 40
SAMPLES_ID_PLOT = sample(1:TOT_SAMPLES, TOT_SPAGUETTIS)
y_pred_mean <- adply(samples$y_pred_mean[SAMPLES_ID_PLOT,], 2)
tmp = melt(y_pred_mean)
names(tmp) = c("xid", "group", "y")
tmp <- mutate(tmp, x=x_pred[xid])


# Median of Bayesian credible intervals

ggplot() +
  geom_point(data=as.data.frame(stan_data),
           aes(x=x,y=y), colour="black", size=2)  +
  geom_line(data=df_pred, aes(x=x_pred, y=ymedian), colour="blue", size=1.5,
            linetype='dashed') +
  theme_bw() -> lm_plot_base
print(lm_plot_base)

# 95% Bayesian credible intervals
lm_plot = lm_plot_base +  
  geom_ribbon(data=df_pred, aes(x=x_pred, ymin=ymin, ymax=ymax), 
              alpha=0.1)
print(lm_plot)

#Spaghetti plots: samples of individual fitted lines
lm_plot = lm_plot +  
  geom_line(data=tmp, aes(x=x,y=y, group=group), colour="#999999", alpha=0.3)
print(lm_plot)


# `lm` fitted line
lm_plot = lm_plot_base +  
  geom_line(data = df_pred, aes(x=x_pred, y=y_pred_lm), colour="black", size=4,
            alpha=0.3) +
  geom_ribbon(data=df_pred, aes(x=x_pred, ymin=ymin, ymax=ymax), 
              alpha=0.1)
print(lm_plot)


# stan_data
# 
# plot(cars)
# lines(x_pred, mean(samples$beta0) + mean(samples$beta1)*x_pred)
# 
# lines(x_pred, colMeans(samples$y_pred))
# lines(x_pred, apply(samples$y_pred, 2,quantile, probs=(0.05)))
# lines(x_pred, apply(samples$y_pred, 2,quantile, probs=(0.975)))
# 
# quantile(samples$y_pred, probs=c(0.25,0.5))
# 
# stan_lm_pred_summ = summary(stan_lm_pred)
# summ = stan_lm_pred_summ$c_summary
# 
# colnames(summ)
# typeof(summ)
# summ[1]

```





